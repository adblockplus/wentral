{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"A frontend for machine learning models that detect objects in web pages, that can be used to: Measure and visualize the performance of object detectors on datasets, Expose an object detector as an HTTP web service. Installation We recommend to install Wentral into a virtualenv. After the virtualenv is activated execute python setup.py install . Dependencies will be installed automatically. Web service To make a web service from a detector.Class loading the weights from weights/file run: $ wentral ws -d detector.Class -w weights/file See CLI docs for more info on usage. Benchmarks To benchmark detector.Class on a data/set run: $ wentral bm [-v] -d detector.Class -w weights/file data/set See CLI docs for more info on usage. Development Most common scenario will be implementing detectors to use with Wentral. The API docs has more detail on this. You are also welcome to contribute to Wentral itself. Make sure the tests still pass and the coverage is not reduced. Make sure to follow eyeo coding style to make reviews simpler. Testing Python We use Tox for testing Python code and Python linting. Install Tox with pip if you haven't already and then run the tests with: $ tox JavaScript There's also a small amount of JavaScript in this repo. Unfortunately it has no tests, but you can lint it using ESLint (more info on eyeo eslint config here ). Make sure you have ESLint and eyeo config installed: $ npm install -g eslint eslint-config-eyeo and then run: $ eslint wentral/vis_ui/visualization.js You only need to do it if you changed that file. CI The CI setup in the GitLab repository runs both Python tests and JavaScript linting. It's configured via .gitlab-ci.yml . License Wentral is Free and Open Source software distributed under the terms of MIT license (see LICENSE.txt for more details).","title":"Overview"},{"location":"#installation","text":"We recommend to install Wentral into a virtualenv. After the virtualenv is activated execute python setup.py install . Dependencies will be installed automatically.","title":"Installation"},{"location":"#web-service","text":"To make a web service from a detector.Class loading the weights from weights/file run: $ wentral ws -d detector.Class -w weights/file See CLI docs for more info on usage.","title":"Web service"},{"location":"#benchmarks","text":"To benchmark detector.Class on a data/set run: $ wentral bm [-v] -d detector.Class -w weights/file data/set See CLI docs for more info on usage.","title":"Benchmarks"},{"location":"#development","text":"Most common scenario will be implementing detectors to use with Wentral. The API docs has more detail on this. You are also welcome to contribute to Wentral itself. Make sure the tests still pass and the coverage is not reduced. Make sure to follow eyeo coding style to make reviews simpler.","title":"Development"},{"location":"#testing","text":"","title":"Testing"},{"location":"#python","text":"We use Tox for testing Python code and Python linting. Install Tox with pip if you haven't already and then run the tests with: $ tox","title":"Python"},{"location":"#javascript","text":"There's also a small amount of JavaScript in this repo. Unfortunately it has no tests, but you can lint it using ESLint (more info on eyeo eslint config here ). Make sure you have ESLint and eyeo config installed: $ npm install -g eslint eslint-config-eyeo and then run: $ eslint wentral/vis_ui/visualization.js You only need to do it if you changed that file.","title":"JavaScript"},{"location":"#ci","text":"The CI setup in the GitLab repository runs both Python tests and JavaScript linting. It's configured via .gitlab-ci.yml .","title":"CI"},{"location":"#license","text":"Wentral is Free and Open Source software distributed under the terms of MIT license (see LICENSE.txt for more details).","title":"License"},{"location":"api/","text":"Wentral API This document describes parts of Wentral API that is related to implementing detectors as well as the web service HTTP API and the Python library for working with it. Implementing detectors Detector API is specified in wentral.detector . This module also provides a base class for detectors (although detector implementations are not required to inherit it as long as they implement the API). Basically detectors are expected to do three things: Accept detector parameters as arguments to __init__ . The most common parameters are weights_file -- a path to the file containing the model weights, confidence_threshold and iou_threshold . There are no particular constraints on these parameters other than that it's convenient to use standard names provided by Wentral because it makes it easier to pass them from the command line. If a argument of __init__ has type signature, it will be used by Wentral to convert the argument value before passing it to the constructor. Implement reasonable __str__ method that includes any important parameters. Implement detect and batch_detect methods ( Detector base class provides an implementation of batch_detect that calls detect in a loop but implementations are encouraged to do batch_detect in parallel when possible). detect method This method takes one image and returns regions detected in it. The arguments of detect are: image -- PIL.Image object that contains the image for region detection. path -- Path/name of the image. Some detectors (e.g. json and static ) use the path instead of the actual image but ML models typically ignore it. It can also take additional arguments that override detector parameters for this particular call, such as confidence_threshold and iou_threshold . The return value of detect should be a list of tuples that contains box coordinates and detection confidence. batch_detect method Batch detect does the same for a batch of images. It takes a list of tuples of image and path plus same additional arguments as detect . The return value should be an iterator over (path, detections) tuples where path is the original image path and detections is a list of detections in the same format as for detect . The order of the return value doesn't have to be the same as the order of the input. Depending on implementation details this might be a list or a generator: Wentral is agnostic about this to give flexibility to the detector implementation. Using the web service Client code for Python is provided in wentral.client : from PIL import Image import wentral.client as cl detector = cl.ProxyDetector('http://localhost:8080/') path = 'path/image.png' image = Image.open(path) boxes = detector.detect(image, path) To use the web service directly, upload the image to http://host:port/detect using a POST request with content type multipart/form-data . The field name should be called image . The result is returned as a JSON document that contains an object with the following keys: size -- An array with width and height of the image. detection_time -- Detection time in seconds. boxes -- Array of arrays that contain detection box coordinates and detection confidence. The requests to detect endpoint can also include additional parameters for the detection process (they are sent as URL parameters): confidence_threshold - minimum model confidence for detections to be returned (default: 0.5). iou_threshold - IOU above which two detections are considered duplicate and only the highest confidence one will be returned (default: 0.4). slicing_threshold - aspect ratio (short side over long side) below which the image will be cut into square slices as the model doesn't deal well with very non-square images (default: 0.7). slice_overlap - minimal ratio of the slice area that will be overlapped: overlaps are necessary to make sure the objects at slice boundaries get detected (default: 0.2).","title":"API"},{"location":"api/#wentral-api","text":"This document describes parts of Wentral API that is related to implementing detectors as well as the web service HTTP API and the Python library for working with it.","title":"Wentral API"},{"location":"api/#implementing-detectors","text":"Detector API is specified in wentral.detector . This module also provides a base class for detectors (although detector implementations are not required to inherit it as long as they implement the API). Basically detectors are expected to do three things: Accept detector parameters as arguments to __init__ . The most common parameters are weights_file -- a path to the file containing the model weights, confidence_threshold and iou_threshold . There are no particular constraints on these parameters other than that it's convenient to use standard names provided by Wentral because it makes it easier to pass them from the command line. If a argument of __init__ has type signature, it will be used by Wentral to convert the argument value before passing it to the constructor. Implement reasonable __str__ method that includes any important parameters. Implement detect and batch_detect methods ( Detector base class provides an implementation of batch_detect that calls detect in a loop but implementations are encouraged to do batch_detect in parallel when possible).","title":"Implementing detectors"},{"location":"api/#detect-method","text":"This method takes one image and returns regions detected in it. The arguments of detect are: image -- PIL.Image object that contains the image for region detection. path -- Path/name of the image. Some detectors (e.g. json and static ) use the path instead of the actual image but ML models typically ignore it. It can also take additional arguments that override detector parameters for this particular call, such as confidence_threshold and iou_threshold . The return value of detect should be a list of tuples that contains box coordinates and detection confidence.","title":"detect method"},{"location":"api/#batch_detect-method","text":"Batch detect does the same for a batch of images. It takes a list of tuples of image and path plus same additional arguments as detect . The return value should be an iterator over (path, detections) tuples where path is the original image path and detections is a list of detections in the same format as for detect . The order of the return value doesn't have to be the same as the order of the input. Depending on implementation details this might be a list or a generator: Wentral is agnostic about this to give flexibility to the detector implementation.","title":"batch_detect method"},{"location":"api/#using-the-web-service","text":"Client code for Python is provided in wentral.client : from PIL import Image import wentral.client as cl detector = cl.ProxyDetector('http://localhost:8080/') path = 'path/image.png' image = Image.open(path) boxes = detector.detect(image, path) To use the web service directly, upload the image to http://host:port/detect using a POST request with content type multipart/form-data . The field name should be called image . The result is returned as a JSON document that contains an object with the following keys: size -- An array with width and height of the image. detection_time -- Detection time in seconds. boxes -- Array of arrays that contain detection box coordinates and detection confidence. The requests to detect endpoint can also include additional parameters for the detection process (they are sent as URL parameters): confidence_threshold - minimum model confidence for detections to be returned (default: 0.5). iou_threshold - IOU above which two detections are considered duplicate and only the highest confidence one will be returned (default: 0.4). slicing_threshold - aspect ratio (short side over long side) below which the image will be cut into square slices as the model doesn't deal well with very non-square images (default: 0.7). slice_overlap - minimal ratio of the slice area that will be overlapped: overlaps are necessary to make sure the objects at slice boundaries get detected (default: 0.2).","title":"Using the web service"},{"location":"cli/","text":"Wentral command line usage Wentral contains a command line wrapper (called wentral ) that allows access to most features. When installing Wentral with pip install it will be placed into the bin/ directory where Python and other script wrappers live. The CLI has two subcommands that are described below. Web service Wentral can expose any detector implementation as a web service that can be used to perform multiple benchmarking runs without reloading the model or as part of production setup. The basic invocation goes like this: $ wentral ws -d DETECTOR_CLASS -w WEIGHTS_FILE [--port PORT] The web service will listen on specified PORT (8080 by default), and you can interact with it by pointing your browser to http://localhost:8080/ or programmatically . See wentral ws -h for description of additional options or see detector arguments description below. There's also a GET endpoint for requesting server status at http://localhost:8080/status . It returns a JSON document that contains the information about server memory consumption and current active detection requests (including their parameters). Slicing proxy What wentral ws exposes is actually not the detector class itself. Instead it wraps it with SlicindDetectorProxy from wentral.slicing_detector_proxy . The proxy can deal with screenshots that are very far from square share by slicing them into square fragments, running detection on fragments (it calls batch_detect method of the original detector class) and then combining the detections from the fragments into detections from the whole screenshot. To configure the slicing, wentral ws takes the following additional parameters: --slicing-threshold -- If the aspect ratio (short axis, typically width, divided by long axis, typically height) of the screenshot is equal or less than this number, it will be sliced. Otherwise slicing proxy just passes it through to the wrapped detector. The default value for this is 0.7. --slice-overlap -- When non-square images are cut into slices, the slices will overlap each other by some percentage of their area. This parameter configures the overlap percentage. Default value is 0.2. Benchmarking Wentral can also be used to measure the performance of detector implementations : $ wentral bm -d DETECTOR_CLASS DATASET_PATH You will need to supply detector-specific parameters like --weights-file / -w to initialize the detector class. Run wentral bm -h to get more help on those or see detector arguments description below. Wentral will load the images from DATASET_PATH . It will also load the ground truth from a CSV file in the same directory or from TXT files (in YOLOv3) format) that have the same names as the images. Benchmark runs output overall statistics to standard output. You can request numbers on individual images using --verbose / -v and/or more detailed JSON output via --output or -o . Other command line arguments that affect benchmarking are: --confidence-threshold / -c -- Only detections with confidence equal or greater than this are counted for the purposes of calculating true and false positives, false negatives, precision, recall and f1 score. For mAP all detections are used, they are also all stored in JSON output if it's requested. The default confidence threshold is 0.5. --match-iou / -m -- Only detections that have sufficient overlap with the ground truth boxes are counted as true positives. Overlap is measured by intersection over union (IoU) and this argument sets IoU value at which detections are considered to be matching the ground truth. The default value for this argument is 0.4. --visualizations-path / -z -- Create a visualization of detections and ground truth boxes and save it in the directory specified by this parameter. Visualization When visualization is requested, wentral bm will create an additional visualization folder. The folder will contain index.html , some JavaScript and JSON and some images. The UI can be viewed locally with python -m http.server or hosted at a more serious webserver, or cloud static hosting like Amazon S3 or Google Cloud Storage. Then just point your browser to index.html . There are two sections in the visualization UI: screenshots and detections. Screenshots This section shows full screenshots with detection boxes drawn on top of them. The colors of the displayed boxes will be as follows: Detection matching a ground truth box (true positive): green, Detection not matching a ground truth box (false positive): orange, Ground truth box matching a detection (also true positive): blue, Ground truth box not matching a detection (false negative): red. All detections also have their corresponding confidence displayed on top of them in %. You can adjust the size of screenshots using the slider at the top, or zoom in with a mouse click on the screenshot. It's possible to filter the screenshots to only see the ones that have false positives or false negatives in them. Detections This section shows detections and ground truth boxes: Ground truth boxes that have been detected have a blue frame, Detections matching a ground truth box have a green frame, Detections not matching a ground truth box (false positive) have an orange frame, Undetected ground truth boxes (false negatives) have a red frame. Similarly to the screenshots view, it's possible to zoom in by clicking on detections. The zoom in view also shows the detection or ground truth box in context of the screenshot that it comes from. Similarity data An additional file with detection/ground truth similarity information can be placed into the same directory named nn.json . When similarity information is provided, similar images are displayed in the zoom in view below the full screenshot (for those fragments for which similarity is provided). Detector arguments Most detectors are Python classes that wrap machine learning models, usually neural networks. The arguments that are normally provided to them are: --weights-file / -w -- path of the file that contains model weights. --iou-threshold -- Usually the models discard detections that have too much overlap with other detections. Overlap is measured by intersection over union (IoU) and this argument sets IoU value at which detections are considered to be duplicates. The default value for this is 0.4. In addition, you can use --extra / -x (the parameter takes the form ARGNAME=VALUE ) to pass additional parameters to the constructor of the detector class. Special detectors In addition to ML-based detectors, there are 3 built-in special detectors that can be specified as the value of -d argument: json -- This detector needs --path / -p argument pointing to a JSON file that was earlier produced by wentral bm ... -o JSON_FILE . It will load detections from this JSON file and it allows recalculating the results with different confidence threshold and match IoU values. server -- This detector needs --server-url / -s argument with URL of a server that runs wentral ws . Mostly useful for running several benchmarks on the same model without reloading the weights. static -- This detector needs --path / -p argument that points to a directory containing a dataset (in the same format as DATASET ). The detector returns marked regions in that dataset as detections. Of course if the two datasets must contain the same images. This is useful for evaluating datasets labeled by humans to establish a human level baseline.","title":"Command line usage"},{"location":"cli/#wentral-command-line-usage","text":"Wentral contains a command line wrapper (called wentral ) that allows access to most features. When installing Wentral with pip install it will be placed into the bin/ directory where Python and other script wrappers live. The CLI has two subcommands that are described below.","title":"Wentral command line usage"},{"location":"cli/#web-service","text":"Wentral can expose any detector implementation as a web service that can be used to perform multiple benchmarking runs without reloading the model or as part of production setup. The basic invocation goes like this: $ wentral ws -d DETECTOR_CLASS -w WEIGHTS_FILE [--port PORT] The web service will listen on specified PORT (8080 by default), and you can interact with it by pointing your browser to http://localhost:8080/ or programmatically . See wentral ws -h for description of additional options or see detector arguments description below. There's also a GET endpoint for requesting server status at http://localhost:8080/status . It returns a JSON document that contains the information about server memory consumption and current active detection requests (including their parameters).","title":"Web service"},{"location":"cli/#slicing-proxy","text":"What wentral ws exposes is actually not the detector class itself. Instead it wraps it with SlicindDetectorProxy from wentral.slicing_detector_proxy . The proxy can deal with screenshots that are very far from square share by slicing them into square fragments, running detection on fragments (it calls batch_detect method of the original detector class) and then combining the detections from the fragments into detections from the whole screenshot. To configure the slicing, wentral ws takes the following additional parameters: --slicing-threshold -- If the aspect ratio (short axis, typically width, divided by long axis, typically height) of the screenshot is equal or less than this number, it will be sliced. Otherwise slicing proxy just passes it through to the wrapped detector. The default value for this is 0.7. --slice-overlap -- When non-square images are cut into slices, the slices will overlap each other by some percentage of their area. This parameter configures the overlap percentage. Default value is 0.2.","title":"Slicing proxy"},{"location":"cli/#benchmarking","text":"Wentral can also be used to measure the performance of detector implementations : $ wentral bm -d DETECTOR_CLASS DATASET_PATH You will need to supply detector-specific parameters like --weights-file / -w to initialize the detector class. Run wentral bm -h to get more help on those or see detector arguments description below. Wentral will load the images from DATASET_PATH . It will also load the ground truth from a CSV file in the same directory or from TXT files (in YOLOv3) format) that have the same names as the images. Benchmark runs output overall statistics to standard output. You can request numbers on individual images using --verbose / -v and/or more detailed JSON output via --output or -o . Other command line arguments that affect benchmarking are: --confidence-threshold / -c -- Only detections with confidence equal or greater than this are counted for the purposes of calculating true and false positives, false negatives, precision, recall and f1 score. For mAP all detections are used, they are also all stored in JSON output if it's requested. The default confidence threshold is 0.5. --match-iou / -m -- Only detections that have sufficient overlap with the ground truth boxes are counted as true positives. Overlap is measured by intersection over union (IoU) and this argument sets IoU value at which detections are considered to be matching the ground truth. The default value for this argument is 0.4. --visualizations-path / -z -- Create a visualization of detections and ground truth boxes and save it in the directory specified by this parameter.","title":"Benchmarking"},{"location":"cli/#visualization","text":"When visualization is requested, wentral bm will create an additional visualization folder. The folder will contain index.html , some JavaScript and JSON and some images. The UI can be viewed locally with python -m http.server or hosted at a more serious webserver, or cloud static hosting like Amazon S3 or Google Cloud Storage. Then just point your browser to index.html . There are two sections in the visualization UI: screenshots and detections.","title":"Visualization"},{"location":"cli/#screenshots","text":"This section shows full screenshots with detection boxes drawn on top of them. The colors of the displayed boxes will be as follows: Detection matching a ground truth box (true positive): green, Detection not matching a ground truth box (false positive): orange, Ground truth box matching a detection (also true positive): blue, Ground truth box not matching a detection (false negative): red. All detections also have their corresponding confidence displayed on top of them in %. You can adjust the size of screenshots using the slider at the top, or zoom in with a mouse click on the screenshot. It's possible to filter the screenshots to only see the ones that have false positives or false negatives in them.","title":"Screenshots"},{"location":"cli/#detections","text":"This section shows detections and ground truth boxes: Ground truth boxes that have been detected have a blue frame, Detections matching a ground truth box have a green frame, Detections not matching a ground truth box (false positive) have an orange frame, Undetected ground truth boxes (false negatives) have a red frame. Similarly to the screenshots view, it's possible to zoom in by clicking on detections. The zoom in view also shows the detection or ground truth box in context of the screenshot that it comes from.","title":"Detections"},{"location":"cli/#similarity-data","text":"An additional file with detection/ground truth similarity information can be placed into the same directory named nn.json . When similarity information is provided, similar images are displayed in the zoom in view below the full screenshot (for those fragments for which similarity is provided).","title":"Similarity data"},{"location":"cli/#detector-arguments","text":"Most detectors are Python classes that wrap machine learning models, usually neural networks. The arguments that are normally provided to them are: --weights-file / -w -- path of the file that contains model weights. --iou-threshold -- Usually the models discard detections that have too much overlap with other detections. Overlap is measured by intersection over union (IoU) and this argument sets IoU value at which detections are considered to be duplicates. The default value for this is 0.4. In addition, you can use --extra / -x (the parameter takes the form ARGNAME=VALUE ) to pass additional parameters to the constructor of the detector class.","title":"Detector arguments"},{"location":"cli/#special-detectors","text":"In addition to ML-based detectors, there are 3 built-in special detectors that can be specified as the value of -d argument: json -- This detector needs --path / -p argument pointing to a JSON file that was earlier produced by wentral bm ... -o JSON_FILE . It will load detections from this JSON file and it allows recalculating the results with different confidence threshold and match IoU values. server -- This detector needs --server-url / -s argument with URL of a server that runs wentral ws . Mostly useful for running several benchmarks on the same model without reloading the weights. static -- This detector needs --path / -p argument that points to a directory containing a dataset (in the same format as DATASET ). The detector returns marked regions in that dataset as detections. Of course if the two datasets must contain the same images. This is useful for evaluating datasets labeled by humans to establish a human level baseline.","title":"Special detectors"},{"location":"file-formats/","text":"Wentral file formats Wentral creates and consumes several file formats that are described below. Dataset regions Datasets generally come as a directory of images that contain an additional file or files with region markings. Region markings come in CSV or TXT formats. CSV CSV region markings are a single file that contains the following columns: image -- file name of the image to which the region belongs. xmin , ymin , xmax , ymax -- bounding box of the region; the coordinates are from 0 to width / height - 1. label -- a text string, indicating the type for this region. TXT (YOLO format) TXT region markings are one file per image, that has the same name as the image but with .txt extension (instead of .png or .jpg ). The file contains one line per region, that has the following format: TYPE X Y WIDTH HEIGHT Here TYPE is a non-negative integer that indicates the region type, X and Y are floats between 0 and 1 that are coordinates of the center of the region and WIDTH and HEIGHT are floats between 0 and 1 that are width and height of the region. All sizes and coordinates are relative to the image size. TXT region files normally come accompanied by region.names file that contains the names of the region types: the first line corresponds to TYPE 0, second line to TYPE 1 and so forth. JSON output of benchmarks Benchmarking detectors with wentral bm ... -o output.json produces a detailed results file in JSON format. The content of the file is an object with the following keys: dataset -- Dataset description. detector -- Detector and its parameters. image_count -- Number of images in the dataset. images_path -- Path to the dataset directory. tp -- True positives count. fp -- False positives count. fn -- False negatives count. precision , recall , f1 -- Precision, recall and F1 score. mAP -- Mean average precision (it's actually simple average precision because Wentral only has one class or regions). images -- Array of objects that contain information about individual images. Each object contains the following keys: image_name -- File name of the image. confidence_threshold -- Confidence threshold that's used to filter the detections before they are compared to the ground truth. match_iou -- IoU necessary for a detection to match a ground truth box. tp , fp , fn , precision , recall , f1 -- Results for this one image. detections -- Array of arrays that contain coordinates of detections (X0, Y0, X1, Y1) followed by detection confidence and true / false depending on whether this detection matches a ground truth box. Note that only detections with confidence over the threshold are counted for evaulating model performance. ground_truth -- Array of arrays that contain coordinates of ground truth boxes (X0, Y0, X1, Y1) followed by detection confidence. If there are no matching detections, detection confidence will be 0. Output of benchmark visualization Running wentral bm ... -z vis-dir produces a directory with visualization UI. It consists of the original images with detection boxes drawn on them, smaller images of detections and ground truth boxes, index.html that contains the HTML of the UI, visualization.js that contains the logic and data.js that contains the data. data.json contains a list of objects with the following keys: name -- File name of the image. tp , fp , fn -- Number of true / false positives and false negatives. detections -- Object with two keys: true and false , that each contain an array of object with the following keys: file -- File name of the detection image. box -- Array that contains the box coordinates and detection confidence. ground_truth -- Object with two keys: detected and missed , that each contain an array of objects with the following keys: file -- File name of the detection image. box -- Array that contains the box coordinates and detection confidence (detection confidence might be 0). Detection similarity data Visualization UI can also display ground truth boxes similar to false positives and false negatives. For this a detection similarity file should be placed in the visualization directory. It should be named nn.json and contain an object that maps file names of the detections / ground truth images to an array of objects with the following keys: file -- File name of the similar image. dist -- Distance between the original image and the similar one.","title":"File Formats"},{"location":"file-formats/#wentral-file-formats","text":"Wentral creates and consumes several file formats that are described below.","title":"Wentral file formats"},{"location":"file-formats/#dataset-regions","text":"Datasets generally come as a directory of images that contain an additional file or files with region markings. Region markings come in CSV or TXT formats.","title":"Dataset regions"},{"location":"file-formats/#csv","text":"CSV region markings are a single file that contains the following columns: image -- file name of the image to which the region belongs. xmin , ymin , xmax , ymax -- bounding box of the region; the coordinates are from 0 to width / height - 1. label -- a text string, indicating the type for this region.","title":"CSV"},{"location":"file-formats/#txt-yolo-format","text":"TXT region markings are one file per image, that has the same name as the image but with .txt extension (instead of .png or .jpg ). The file contains one line per region, that has the following format: TYPE X Y WIDTH HEIGHT Here TYPE is a non-negative integer that indicates the region type, X and Y are floats between 0 and 1 that are coordinates of the center of the region and WIDTH and HEIGHT are floats between 0 and 1 that are width and height of the region. All sizes and coordinates are relative to the image size. TXT region files normally come accompanied by region.names file that contains the names of the region types: the first line corresponds to TYPE 0, second line to TYPE 1 and so forth.","title":"TXT (YOLO format)"},{"location":"file-formats/#json-output-of-benchmarks","text":"Benchmarking detectors with wentral bm ... -o output.json produces a detailed results file in JSON format. The content of the file is an object with the following keys: dataset -- Dataset description. detector -- Detector and its parameters. image_count -- Number of images in the dataset. images_path -- Path to the dataset directory. tp -- True positives count. fp -- False positives count. fn -- False negatives count. precision , recall , f1 -- Precision, recall and F1 score. mAP -- Mean average precision (it's actually simple average precision because Wentral only has one class or regions). images -- Array of objects that contain information about individual images. Each object contains the following keys: image_name -- File name of the image. confidence_threshold -- Confidence threshold that's used to filter the detections before they are compared to the ground truth. match_iou -- IoU necessary for a detection to match a ground truth box. tp , fp , fn , precision , recall , f1 -- Results for this one image. detections -- Array of arrays that contain coordinates of detections (X0, Y0, X1, Y1) followed by detection confidence and true / false depending on whether this detection matches a ground truth box. Note that only detections with confidence over the threshold are counted for evaulating model performance. ground_truth -- Array of arrays that contain coordinates of ground truth boxes (X0, Y0, X1, Y1) followed by detection confidence. If there are no matching detections, detection confidence will be 0.","title":"JSON output of benchmarks"},{"location":"file-formats/#output-of-benchmark-visualization","text":"Running wentral bm ... -z vis-dir produces a directory with visualization UI. It consists of the original images with detection boxes drawn on them, smaller images of detections and ground truth boxes, index.html that contains the HTML of the UI, visualization.js that contains the logic and data.js that contains the data. data.json contains a list of objects with the following keys: name -- File name of the image. tp , fp , fn -- Number of true / false positives and false negatives. detections -- Object with two keys: true and false , that each contain an array of object with the following keys: file -- File name of the detection image. box -- Array that contains the box coordinates and detection confidence. ground_truth -- Object with two keys: detected and missed , that each contain an array of objects with the following keys: file -- File name of the detection image. box -- Array that contains the box coordinates and detection confidence (detection confidence might be 0).","title":"Output of benchmark visualization"},{"location":"file-formats/#detection-similarity-data","text":"Visualization UI can also display ground truth boxes similar to false positives and false negatives. For this a detection similarity file should be placed in the visualization directory. It should be named nn.json and contain an object that maps file names of the detections / ground truth images to an array of objects with the following keys: file -- File name of the similar image. dist -- Distance between the original image and the similar one.","title":"Detection similarity data"},{"location":"outline/","text":"Docs outline README Installation Simple usage example How to contribute License Command line usage Web service Benchmarking Visualization API Implementing detectors The web service File formats Datasets Output JSON Visualization JSON Similarity data","title":"Docs outline"},{"location":"outline/#docs-outline","text":"README Installation Simple usage example How to contribute License Command line usage Web service Benchmarking Visualization API Implementing detectors The web service File formats Datasets Output JSON Visualization JSON Similarity data","title":"Docs outline"},{"location":"docs/","text":"A frontend for machine learning models that detect objects in web pages, that can be used to: Measure and visualize the performance of object detectors on datasets, Expose an object detector as an HTTP web service. Installation We recommend to install Wentral into a virtualenv. After the virtualenv is activated execute python setup.py install . Dependencies will be installed automatically. Web service To make a web service from a detector.Class loading the weights from weights/file run: $ wentral ws -d detector.Class -w weights/file See CLI docs for more info on usage. Benchmarks To benchmark detector.Class on a data/set run: $ wentral bm [-v] -d detector.Class -w weights/file data/set See CLI docs for more info on usage. Development Most common scenario will be implementing detectors to use with Wentral. The API docs has more detail on this. You are also welcome to contribute to Wentral itself. Make sure the tests still pass and the coverage is not reduced. Make sure to follow eyeo coding style to make reviews simpler. Testing Python We use Tox for testing Python code and Python linting. Install Tox with pip if you haven't already and then run the tests with: $ tox JavaScript There's also a small amount of JavaScript in this repo. Unfortunately it has no tests, but you can lint it using ESLint (more info on eyeo eslint config here ). Make sure you have ESLint and eyeo config installed: $ npm install -g eslint eslint-config-eyeo and then run: $ eslint wentral/vis_ui/visualization.js You only need to do it if you changed that file. CI The CI setup in the GitLab repository runs both Python tests and JavaScript linting. It's configured via .gitlab-ci.yml . License Wentral is Free and Open Source software distributed under the terms of MIT license (see LICENSE.txt for more details).","title":"Index"},{"location":"docs/#installation","text":"We recommend to install Wentral into a virtualenv. After the virtualenv is activated execute python setup.py install . Dependencies will be installed automatically.","title":"Installation"},{"location":"docs/#web-service","text":"To make a web service from a detector.Class loading the weights from weights/file run: $ wentral ws -d detector.Class -w weights/file See CLI docs for more info on usage.","title":"Web service"},{"location":"docs/#benchmarks","text":"To benchmark detector.Class on a data/set run: $ wentral bm [-v] -d detector.Class -w weights/file data/set See CLI docs for more info on usage.","title":"Benchmarks"},{"location":"docs/#development","text":"Most common scenario will be implementing detectors to use with Wentral. The API docs has more detail on this. You are also welcome to contribute to Wentral itself. Make sure the tests still pass and the coverage is not reduced. Make sure to follow eyeo coding style to make reviews simpler.","title":"Development"},{"location":"docs/#testing","text":"","title":"Testing"},{"location":"docs/#python","text":"We use Tox for testing Python code and Python linting. Install Tox with pip if you haven't already and then run the tests with: $ tox","title":"Python"},{"location":"docs/#javascript","text":"There's also a small amount of JavaScript in this repo. Unfortunately it has no tests, but you can lint it using ESLint (more info on eyeo eslint config here ). Make sure you have ESLint and eyeo config installed: $ npm install -g eslint eslint-config-eyeo and then run: $ eslint wentral/vis_ui/visualization.js You only need to do it if you changed that file.","title":"JavaScript"},{"location":"docs/#ci","text":"The CI setup in the GitLab repository runs both Python tests and JavaScript linting. It's configured via .gitlab-ci.yml .","title":"CI"},{"location":"docs/#license","text":"Wentral is Free and Open Source software distributed under the terms of MIT license (see LICENSE.txt for more details).","title":"License"},{"location":"docs/api/","text":"Wentral API This document describes parts of Wentral API that is related to implementing detectors as well as the web service HTTP API and the Python library for working with it. Implementing detectors Detector API is specified in wentral.detector . This module also provides a base class for detectors (although detector implementations are not required to inherit it as long as they implement the API). Basically detectors are expected to do three things: Accept detector parameters as arguments to __init__ . The most common parameters are weights_file -- a path to the file containing the model weights, confidence_threshold and iou_threshold . There are no particular constraints on these parameters other than that it's convenient to use standard names provided by Wentral because it makes it easier to pass them from the command line. If a argument of __init__ has type signature, it will be used by Wentral to convert the argument value before passing it to the constructor. Implement reasonable __str__ method that includes any important parameters. Implement detect and batch_detect methods ( Detector base class provides an implementation of batch_detect that calls detect in a loop but implementations are encouraged to do batch_detect in parallel when possible). detect method This method takes one image and returns regions detected in it. The arguments of detect are: image -- PIL.Image object that contains the image for region detection. path -- Path/name of the image. Some detectors (e.g. json and static ) use the path instead of the actual image but ML models typically ignore it. It can also take additional arguments that override detector parameters for this particular call, such as confidence_threshold and iou_threshold . The return value of detect should be a list of tuples that contains box coordinates and detection confidence. batch_detect method Batch detect does the same for a batch of images. It takes a list of tuples of image and path plus same additional arguments as detect . The return value should be an iterator over (path, detections) tuples where path is the original image path and detections is a list of detections in the same format as for detect . The order of the return value doesn't have to be the same as the order of the input. Depending on implementation details this might be a list or a generator: Wentral is agnostic about this to give flexibility to the detector implementation. Using the web service Client code for Python is provided in wentral.client : from PIL import Image import wentral.client as cl detector = cl.ProxyDetector('http://localhost:8080/') path = 'path/image.png' image = Image.open(path) boxes = detector.detect(image, path) To use the web service directly, upload the image to http://host:port/detect using a POST request with content type multipart/form-data . The field name should be called image . The result is returned as a JSON document that contains an object with the following keys: size -- An array with width and height of the image. detection_time -- Detection time in seconds. boxes -- Array of arrays that contain detection box coordinates and detection confidence. The requests to detect endpoint can also include additional parameters for the detection process (they are sent as URL parameters): confidence_threshold - minimum model confidence for detections to be returned (default: 0.5). iou_threshold - IOU above which two detections are considered duplicate and only the highest confidence one will be returned (default: 0.4). slicing_threshold - aspect ratio (short side over long side) below which the image will be cut into square slices as the model doesn't deal well with very non-square images (default: 0.7). slice_overlap - minimal ratio of the slice area that will be overlapped: overlaps are necessary to make sure the objects at slice boundaries get detected (default: 0.2).","title":"Wentral API"},{"location":"docs/api/#wentral-api","text":"This document describes parts of Wentral API that is related to implementing detectors as well as the web service HTTP API and the Python library for working with it.","title":"Wentral API"},{"location":"docs/api/#implementing-detectors","text":"Detector API is specified in wentral.detector . This module also provides a base class for detectors (although detector implementations are not required to inherit it as long as they implement the API). Basically detectors are expected to do three things: Accept detector parameters as arguments to __init__ . The most common parameters are weights_file -- a path to the file containing the model weights, confidence_threshold and iou_threshold . There are no particular constraints on these parameters other than that it's convenient to use standard names provided by Wentral because it makes it easier to pass them from the command line. If a argument of __init__ has type signature, it will be used by Wentral to convert the argument value before passing it to the constructor. Implement reasonable __str__ method that includes any important parameters. Implement detect and batch_detect methods ( Detector base class provides an implementation of batch_detect that calls detect in a loop but implementations are encouraged to do batch_detect in parallel when possible).","title":"Implementing detectors"},{"location":"docs/api/#detect-method","text":"This method takes one image and returns regions detected in it. The arguments of detect are: image -- PIL.Image object that contains the image for region detection. path -- Path/name of the image. Some detectors (e.g. json and static ) use the path instead of the actual image but ML models typically ignore it. It can also take additional arguments that override detector parameters for this particular call, such as confidence_threshold and iou_threshold . The return value of detect should be a list of tuples that contains box coordinates and detection confidence.","title":"detect method"},{"location":"docs/api/#batch_detect-method","text":"Batch detect does the same for a batch of images. It takes a list of tuples of image and path plus same additional arguments as detect . The return value should be an iterator over (path, detections) tuples where path is the original image path and detections is a list of detections in the same format as for detect . The order of the return value doesn't have to be the same as the order of the input. Depending on implementation details this might be a list or a generator: Wentral is agnostic about this to give flexibility to the detector implementation.","title":"batch_detect method"},{"location":"docs/api/#using-the-web-service","text":"Client code for Python is provided in wentral.client : from PIL import Image import wentral.client as cl detector = cl.ProxyDetector('http://localhost:8080/') path = 'path/image.png' image = Image.open(path) boxes = detector.detect(image, path) To use the web service directly, upload the image to http://host:port/detect using a POST request with content type multipart/form-data . The field name should be called image . The result is returned as a JSON document that contains an object with the following keys: size -- An array with width and height of the image. detection_time -- Detection time in seconds. boxes -- Array of arrays that contain detection box coordinates and detection confidence. The requests to detect endpoint can also include additional parameters for the detection process (they are sent as URL parameters): confidence_threshold - minimum model confidence for detections to be returned (default: 0.5). iou_threshold - IOU above which two detections are considered duplicate and only the highest confidence one will be returned (default: 0.4). slicing_threshold - aspect ratio (short side over long side) below which the image will be cut into square slices as the model doesn't deal well with very non-square images (default: 0.7). slice_overlap - minimal ratio of the slice area that will be overlapped: overlaps are necessary to make sure the objects at slice boundaries get detected (default: 0.2).","title":"Using the web service"},{"location":"docs/cli/","text":"Wentral command line usage Wentral contains a command line wrapper (called wentral ) that allows access to most features. When installing Wentral with pip install it will be placed into the bin/ directory where Python and other script wrappers live. The CLI has two subcommands that are described below. Web service Wentral can expose any detector implementation as a web service that can be used to perform multiple benchmarking runs without reloading the model or as part of production setup. The basic invocation goes like this: $ wentral ws -d DETECTOR_CLASS -w WEIGHTS_FILE [--port PORT] The web service will listen on specified PORT (8080 by default), and you can interact with it by pointing your browser to http://localhost:8080/ or programmatically . See wentral ws -h for description of additional options or see detector arguments description below. There's also a GET endpoint for requesting server status at http://localhost:8080/status . It returns a JSON document that contains the information about server memory consumption and current active detection requests (including their parameters). Slicing proxy What wentral ws exposes is actually not the detector class itself. Instead it wraps it with SlicindDetectorProxy from wentral.slicing_detector_proxy . The proxy can deal with screenshots that are very far from square share by slicing them into square fragments, running detection on fragments (it calls batch_detect method of the original detector class) and then combining the detections from the fragments into detections from the whole screenshot. To configure the slicing, wentral ws takes the following additional parameters: --slicing-threshold -- If the aspect ratio (short axis, typically width, divided by long axis, typically height) of the screenshot is equal or less than this number, it will be sliced. Otherwise slicing proxy just passes it through to the wrapped detector. The default value for this is 0.7. --slice-overlap -- When non-square images are cut into slices, the slices will overlap each other by some percentage of their area. This parameter configures the overlap percentage. Default value is 0.2. Benchmarking Wentral can also be used to measure the performance of detector implementations : $ wentral bm -d DETECTOR_CLASS DATASET_PATH You will need to supply detector-specific parameters like --weights-file / -w to initialize the detector class. Run wentral bm -h to get more help on those or see detector arguments description below. Wentral will load the images from DATASET_PATH . It will also load the ground truth from a CSV file in the same directory or from TXT files (in YOLOv3) format) that have the same names as the images. Benchmark runs output overall statistics to standard output. You can request numbers on individual images using --verbose / -v and/or more detailed JSON output via --output or -o . Other command line arguments that affect benchmarking are: --confidence-threshold / -c -- Only detections with confidence equal or greater than this are counted for the purposes of calculating true and false positives, false negatives, precision, recall and f1 score. For mAP all detections are used, they are also all stored in JSON output if it's requested. The default confidence threshold is 0.5. --match-iou / -m -- Only detections that have sufficient overlap with the ground truth boxes are counted as true positives. Overlap is measured by intersection over union (IoU) and this argument sets IoU value at which detections are considered to be matching the ground truth. The default value for this argument is 0.4. --visualizations-path / -z -- Create a visualization of detections and ground truth boxes and save it in the directory specified by this parameter. Visualization When visualization is requested, wentral bm will create an additional visualization folder. The folder will contain index.html , some JavaScript and JSON and some images. The UI can be viewed locally with python -m http.server or hosted at a more serious webserver, or cloud static hosting like Amazon S3 or Google Cloud Storage. Then just point your browser to index.html . There are two sections in the visualization UI: screenshots and detections. Screenshots This section shows full screenshots with detection boxes drawn on top of them. The colors of the displayed boxes will be as follows: Detection matching a ground truth box (true positive): green, Detection not matching a ground truth box (false positive): orange, Ground truth box matching a detection (also true positive): blue, Ground truth box not matching a detection (false negative): red. All detections also have their corresponding confidence displayed on top of them in %. You can adjust the size of screenshots using the slider at the top, or zoom in with a mouse click on the screenshot. It's possible to filter the screenshots to only see the ones that have false positives or false negatives in them. Detections This section shows detections and ground truth boxes: Ground truth boxes that have been detected have a blue frame, Detections matching a ground truth box have a green frame, Detections not matching a ground truth box (false positive) have an orange frame, Undetected ground truth boxes (false negatives) have a red frame. Similarly to the screenshots view, it's possible to zoom in by clicking on detections. The zoom in view also shows the detection or ground truth box in context of the screenshot that it comes from. Similarity data An additional file with detection/ground truth similarity information can be placed into the same directory named nn.json . When similarity information is provided, similar images are displayed in the zoom in view below the full screenshot (for those fragments for which similarity is provided). Detector arguments Most detectors are Python classes that wrap machine learning models, usually neural networks. The arguments that are normally provided to them are: --weights-file / -w -- path of the file that contains model weights. --iou-threshold -- Usually the models discard detections that have too much overlap with other detections. Overlap is measured by intersection over union (IoU) and this argument sets IoU value at which detections are considered to be duplicates. The default value for this is 0.4. In addition, you can use --extra / -x (the parameter takes the form ARGNAME=VALUE ) to pass additional parameters to the constructor of the detector class. Special detectors In addition to ML-based detectors, there are 3 built-in special detectors that can be specified as the value of -d argument: json -- This detector needs --path / -p argument pointing to a JSON file that was earlier produced by wentral bm ... -o JSON_FILE . It will load detections from this JSON file and it allows recalculating the results with different confidence threshold and match IoU values. server -- This detector needs --server-url / -s argument with URL of a server that runs wentral ws . Mostly useful for running several benchmarks on the same model without reloading the weights. static -- This detector needs --path / -p argument that points to a directory containing a dataset (in the same format as DATASET ). The detector returns marked regions in that dataset as detections. Of course if the two datasets must contain the same images. This is useful for evaluating datasets labeled by humans to establish a human level baseline.","title":"Wentral command line usage"},{"location":"docs/cli/#wentral-command-line-usage","text":"Wentral contains a command line wrapper (called wentral ) that allows access to most features. When installing Wentral with pip install it will be placed into the bin/ directory where Python and other script wrappers live. The CLI has two subcommands that are described below.","title":"Wentral command line usage"},{"location":"docs/cli/#web-service","text":"Wentral can expose any detector implementation as a web service that can be used to perform multiple benchmarking runs without reloading the model or as part of production setup. The basic invocation goes like this: $ wentral ws -d DETECTOR_CLASS -w WEIGHTS_FILE [--port PORT] The web service will listen on specified PORT (8080 by default), and you can interact with it by pointing your browser to http://localhost:8080/ or programmatically . See wentral ws -h for description of additional options or see detector arguments description below. There's also a GET endpoint for requesting server status at http://localhost:8080/status . It returns a JSON document that contains the information about server memory consumption and current active detection requests (including their parameters).","title":"Web service"},{"location":"docs/cli/#slicing-proxy","text":"What wentral ws exposes is actually not the detector class itself. Instead it wraps it with SlicindDetectorProxy from wentral.slicing_detector_proxy . The proxy can deal with screenshots that are very far from square share by slicing them into square fragments, running detection on fragments (it calls batch_detect method of the original detector class) and then combining the detections from the fragments into detections from the whole screenshot. To configure the slicing, wentral ws takes the following additional parameters: --slicing-threshold -- If the aspect ratio (short axis, typically width, divided by long axis, typically height) of the screenshot is equal or less than this number, it will be sliced. Otherwise slicing proxy just passes it through to the wrapped detector. The default value for this is 0.7. --slice-overlap -- When non-square images are cut into slices, the slices will overlap each other by some percentage of their area. This parameter configures the overlap percentage. Default value is 0.2.","title":"Slicing proxy"},{"location":"docs/cli/#benchmarking","text":"Wentral can also be used to measure the performance of detector implementations : $ wentral bm -d DETECTOR_CLASS DATASET_PATH You will need to supply detector-specific parameters like --weights-file / -w to initialize the detector class. Run wentral bm -h to get more help on those or see detector arguments description below. Wentral will load the images from DATASET_PATH . It will also load the ground truth from a CSV file in the same directory or from TXT files (in YOLOv3) format) that have the same names as the images. Benchmark runs output overall statistics to standard output. You can request numbers on individual images using --verbose / -v and/or more detailed JSON output via --output or -o . Other command line arguments that affect benchmarking are: --confidence-threshold / -c -- Only detections with confidence equal or greater than this are counted for the purposes of calculating true and false positives, false negatives, precision, recall and f1 score. For mAP all detections are used, they are also all stored in JSON output if it's requested. The default confidence threshold is 0.5. --match-iou / -m -- Only detections that have sufficient overlap with the ground truth boxes are counted as true positives. Overlap is measured by intersection over union (IoU) and this argument sets IoU value at which detections are considered to be matching the ground truth. The default value for this argument is 0.4. --visualizations-path / -z -- Create a visualization of detections and ground truth boxes and save it in the directory specified by this parameter.","title":"Benchmarking"},{"location":"docs/cli/#visualization","text":"When visualization is requested, wentral bm will create an additional visualization folder. The folder will contain index.html , some JavaScript and JSON and some images. The UI can be viewed locally with python -m http.server or hosted at a more serious webserver, or cloud static hosting like Amazon S3 or Google Cloud Storage. Then just point your browser to index.html . There are two sections in the visualization UI: screenshots and detections.","title":"Visualization"},{"location":"docs/cli/#screenshots","text":"This section shows full screenshots with detection boxes drawn on top of them. The colors of the displayed boxes will be as follows: Detection matching a ground truth box (true positive): green, Detection not matching a ground truth box (false positive): orange, Ground truth box matching a detection (also true positive): blue, Ground truth box not matching a detection (false negative): red. All detections also have their corresponding confidence displayed on top of them in %. You can adjust the size of screenshots using the slider at the top, or zoom in with a mouse click on the screenshot. It's possible to filter the screenshots to only see the ones that have false positives or false negatives in them.","title":"Screenshots"},{"location":"docs/cli/#detections","text":"This section shows detections and ground truth boxes: Ground truth boxes that have been detected have a blue frame, Detections matching a ground truth box have a green frame, Detections not matching a ground truth box (false positive) have an orange frame, Undetected ground truth boxes (false negatives) have a red frame. Similarly to the screenshots view, it's possible to zoom in by clicking on detections. The zoom in view also shows the detection or ground truth box in context of the screenshot that it comes from.","title":"Detections"},{"location":"docs/cli/#similarity-data","text":"An additional file with detection/ground truth similarity information can be placed into the same directory named nn.json . When similarity information is provided, similar images are displayed in the zoom in view below the full screenshot (for those fragments for which similarity is provided).","title":"Similarity data"},{"location":"docs/cli/#detector-arguments","text":"Most detectors are Python classes that wrap machine learning models, usually neural networks. The arguments that are normally provided to them are: --weights-file / -w -- path of the file that contains model weights. --iou-threshold -- Usually the models discard detections that have too much overlap with other detections. Overlap is measured by intersection over union (IoU) and this argument sets IoU value at which detections are considered to be duplicates. The default value for this is 0.4. In addition, you can use --extra / -x (the parameter takes the form ARGNAME=VALUE ) to pass additional parameters to the constructor of the detector class.","title":"Detector arguments"},{"location":"docs/cli/#special-detectors","text":"In addition to ML-based detectors, there are 3 built-in special detectors that can be specified as the value of -d argument: json -- This detector needs --path / -p argument pointing to a JSON file that was earlier produced by wentral bm ... -o JSON_FILE . It will load detections from this JSON file and it allows recalculating the results with different confidence threshold and match IoU values. server -- This detector needs --server-url / -s argument with URL of a server that runs wentral ws . Mostly useful for running several benchmarks on the same model without reloading the weights. static -- This detector needs --path / -p argument that points to a directory containing a dataset (in the same format as DATASET ). The detector returns marked regions in that dataset as detections. Of course if the two datasets must contain the same images. This is useful for evaluating datasets labeled by humans to establish a human level baseline.","title":"Special detectors"},{"location":"docs/file-formats/","text":"Wentral file formats Wentral creates and consumes several file formats that are described below. Dataset regions Datasets generally come as a directory of images that contain an additional file or files with region markings. Region markings come in CSV or TXT formats. CSV CSV region markings are a single file that contains the following columns: image -- file name of the image to which the region belongs. xmin , ymin , xmax , ymax -- bounding box of the region; the coordinates are from 0 to width / height - 1. label -- a text string, indicating the type for this region. TXT (YOLO format) TXT region markings are one file per image, that has the same name as the image but with .txt extension (instead of .png or .jpg ). The file contains one line per region, that has the following format: TYPE X Y WIDTH HEIGHT Here TYPE is a non-negative integer that indicates the region type, X and Y are floats between 0 and 1 that are coordinates of the center of the region and WIDTH and HEIGHT are floats between 0 and 1 that are width and height of the region. All sizes and coordinates are relative to the image size. TXT region files normally come accompanied by region.names file that contains the names of the region types: the first line corresponds to TYPE 0, second line to TYPE 1 and so forth. JSON output of benchmarks Benchmarking detectors with wentral bm ... -o output.json produces a detailed results file in JSON format. The content of the file is an object with the following keys: dataset -- Dataset description. detector -- Detector and its parameters. image_count -- Number of images in the dataset. images_path -- Path to the dataset directory. tp -- True positives count. fp -- False positives count. fn -- False negatives count. precision , recall , f1 -- Precision, recall and F1 score. mAP -- Mean average precision (it's actually simple average precision because Wentral only has one class or regions). images -- Array of objects that contain information about individual images. Each object contains the following keys: image_name -- File name of the image. confidence_threshold -- Confidence threshold that's used to filter the detections before they are compared to the ground truth. match_iou -- IoU necessary for a detection to match a ground truth box. tp , fp , fn , precision , recall , f1 -- Results for this one image. detections -- Array of arrays that contain coordinates of detections (X0, Y0, X1, Y1) followed by detection confidence and true / false depending on whether this detection matches a ground truth box. Note that only detections with confidence over the threshold are counted for evaulating model performance. ground_truth -- Array of arrays that contain coordinates of ground truth boxes (X0, Y0, X1, Y1) followed by detection confidence. If there are no matching detections, detection confidence will be 0. Output of benchmark visualization Running wentral bm ... -z vis-dir produces a directory with visualization UI. It consists of the original images with detection boxes drawn on them, smaller images of detections and ground truth boxes, index.html that contains the HTML of the UI, visualization.js that contains the logic and data.js that contains the data. data.json contains a list of objects with the following keys: name -- File name of the image. tp , fp , fn -- Number of true / false positives and false negatives. detections -- Object with two keys: true and false , that each contain an array of object with the following keys: file -- File name of the detection image. box -- Array that contains the box coordinates and detection confidence. ground_truth -- Object with two keys: detected and missed , that each contain an array of objects with the following keys: file -- File name of the detection image. box -- Array that contains the box coordinates and detection confidence (detection confidence might be 0). Detection similarity data Visualization UI can also display ground truth boxes similar to false positives and false negatives. For this a detection similarity file should be placed in the visualization directory. It should be named nn.json and contain an object that maps file names of the detections / ground truth images to an array of objects with the following keys: file -- File name of the similar image. dist -- Distance between the original image and the similar one.","title":"Wentral file formats"},{"location":"docs/file-formats/#wentral-file-formats","text":"Wentral creates and consumes several file formats that are described below.","title":"Wentral file formats"},{"location":"docs/file-formats/#dataset-regions","text":"Datasets generally come as a directory of images that contain an additional file or files with region markings. Region markings come in CSV or TXT formats.","title":"Dataset regions"},{"location":"docs/file-formats/#csv","text":"CSV region markings are a single file that contains the following columns: image -- file name of the image to which the region belongs. xmin , ymin , xmax , ymax -- bounding box of the region; the coordinates are from 0 to width / height - 1. label -- a text string, indicating the type for this region.","title":"CSV"},{"location":"docs/file-formats/#txt-yolo-format","text":"TXT region markings are one file per image, that has the same name as the image but with .txt extension (instead of .png or .jpg ). The file contains one line per region, that has the following format: TYPE X Y WIDTH HEIGHT Here TYPE is a non-negative integer that indicates the region type, X and Y are floats between 0 and 1 that are coordinates of the center of the region and WIDTH and HEIGHT are floats between 0 and 1 that are width and height of the region. All sizes and coordinates are relative to the image size. TXT region files normally come accompanied by region.names file that contains the names of the region types: the first line corresponds to TYPE 0, second line to TYPE 1 and so forth.","title":"TXT (YOLO format)"},{"location":"docs/file-formats/#json-output-of-benchmarks","text":"Benchmarking detectors with wentral bm ... -o output.json produces a detailed results file in JSON format. The content of the file is an object with the following keys: dataset -- Dataset description. detector -- Detector and its parameters. image_count -- Number of images in the dataset. images_path -- Path to the dataset directory. tp -- True positives count. fp -- False positives count. fn -- False negatives count. precision , recall , f1 -- Precision, recall and F1 score. mAP -- Mean average precision (it's actually simple average precision because Wentral only has one class or regions). images -- Array of objects that contain information about individual images. Each object contains the following keys: image_name -- File name of the image. confidence_threshold -- Confidence threshold that's used to filter the detections before they are compared to the ground truth. match_iou -- IoU necessary for a detection to match a ground truth box. tp , fp , fn , precision , recall , f1 -- Results for this one image. detections -- Array of arrays that contain coordinates of detections (X0, Y0, X1, Y1) followed by detection confidence and true / false depending on whether this detection matches a ground truth box. Note that only detections with confidence over the threshold are counted for evaulating model performance. ground_truth -- Array of arrays that contain coordinates of ground truth boxes (X0, Y0, X1, Y1) followed by detection confidence. If there are no matching detections, detection confidence will be 0.","title":"JSON output of benchmarks"},{"location":"docs/file-formats/#output-of-benchmark-visualization","text":"Running wentral bm ... -z vis-dir produces a directory with visualization UI. It consists of the original images with detection boxes drawn on them, smaller images of detections and ground truth boxes, index.html that contains the HTML of the UI, visualization.js that contains the logic and data.js that contains the data. data.json contains a list of objects with the following keys: name -- File name of the image. tp , fp , fn -- Number of true / false positives and false negatives. detections -- Object with two keys: true and false , that each contain an array of object with the following keys: file -- File name of the detection image. box -- Array that contains the box coordinates and detection confidence. ground_truth -- Object with two keys: detected and missed , that each contain an array of objects with the following keys: file -- File name of the detection image. box -- Array that contains the box coordinates and detection confidence (detection confidence might be 0).","title":"Output of benchmark visualization"},{"location":"docs/file-formats/#detection-similarity-data","text":"Visualization UI can also display ground truth boxes similar to false positives and false negatives. For this a detection similarity file should be placed in the visualization directory. It should be named nn.json and contain an object that maps file names of the detections / ground truth images to an array of objects with the following keys: file -- File name of the similar image. dist -- Distance between the original image and the similar one.","title":"Detection similarity data"},{"location":"docs/outline/","text":"Docs outline README Installation Simple usage example How to contribute License Command line usage Web service Benchmarking Visualization API Implementing detectors The web service File formats Datasets Output JSON Visualization JSON Similarity data","title":"Docs outline"},{"location":"docs/outline/#docs-outline","text":"README Installation Simple usage example How to contribute License Command line usage Web service Benchmarking Visualization API Implementing detectors The web service File formats Datasets Output JSON Visualization JSON Similarity data","title":"Docs outline"}]}